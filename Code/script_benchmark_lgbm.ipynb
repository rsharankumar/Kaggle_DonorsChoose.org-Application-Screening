{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88a715d0-9520-41b9-85d1-f3ac75ea529f",
    "_uuid": "dffb94b9e76a8dce42cb56728863f1a6e10bea20"
   },
   "source": [
    "# Hi, Welcome to my kernel. We will show you how we achieve 80+% in this challenge. \n",
    "# If you find this helpful, <span style=\"color:#3\">please support us!!!</span>\n",
    "## We will focus on the rate of **<span style=\"color:red\">REJECTED</span>** according to different factors. This may help you to design your model on features that truly matter.\n",
    "![donor](https://i2.wp.com/speechisbeautiful.com/wp-content/uploads/2015/06/donorschoose_logo.png)\n",
    "### DonorsChoose is an US organisation that provide funding to school teachers who wish to improve their education environment. They received approximately thounsands of project proposals every year and the challange they are facing is dealing with enormous of proposal with limited volunteers. We're writing this kernel to help the organizer and participants to filter out insignificant features when designing pre-screening algorithm.\n",
    "### Let's see what we going to analyze in this kernel:\n",
    "1.  Introduction of dataset\n",
    "> Importing the libraries<br/>\n",
    "> Data preparation  <br/>\n",
    "2. Analysis\n",
    "> **Which state has the highest rate of rejected?**<br/>\n",
    "> **What is the relationship between funding amount and rate of rejected? ** <br/>\n",
    "> **Will grade categories affect the rate of rejected?** <br/>\n",
    "> **Which combination of category and sub-category has highest rate of rejected?** <br/>\n",
    "> **What is the relationship between essay sentiment and rate of rejected?** <br/>\n",
    "3. Suggestion\n",
    "4. LGBM + TFIDF\n",
    "5. GRU-ATT\n",
    "6. Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4b47154-d8a3-40a0-b3f9-b8af8d056bd5",
    "_uuid": "80e2d41de00bf26a92126775bc04f8b10333b964"
   },
   "source": [
    "# 1.1 Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46d655fa-7793-436a-b7da-dd5022691f91",
    "_uuid": "34c8baca9ef0c0afd566b59032560037b0e7587f"
   },
   "source": [
    "# 1.2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/donorschoose-application-screening/train.csv')\n",
    "df.sort_values('project_submitted_datetime').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af6fb370-b804-4348-9182-6c66f24e28a7",
    "_uuid": "0b82f96577a417cd6f2a4885e329d8b159cccd54"
   },
   "source": [
    "## In the [train.csv](https://www.kaggle.com/c/donorschoose-application-screening/data), we got total of 16 unique features:\n",
    "* id (unique id, no repeated)\n",
    "* teacher_id (unique id, repeated)\n",
    "* teacher_prefix (Mr. Ms. Mrs. Teacher)\n",
    "* school_state \n",
    "* project_submitted_datetime\n",
    "* project_grade_categories (4 categories)\n",
    "* project_subject_categories (9 categories)\n",
    "* project_subject_subcategories (30 categories)\n",
    "* project_title\n",
    "* project_essay_1\n",
    "* project_essay_2\n",
    "* project_essay_3\n",
    "* project_essay_4\n",
    "* project_resource_summary\n",
    "* teacher_number_of_previously_posted_projects\n",
    "* project_is_approved (0 - Rejected , 1 - Approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7796e5ef-f294-4ec6-af8c-fa16fe03623d",
    "_kg_hide-input": false,
    "_uuid": "edd7e4e763f63f18f2851aedbdcb191179982225",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resources_df = pd.read_csv('../input/donorschoose-application-screening/resources.csv')\n",
    "resources_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db8e2edf-fd58-415d-8ed5-a7a83854a6e1",
    "_uuid": "7039763f2e0875e9d029e76c10022d9ef399ffb7"
   },
   "source": [
    "## In the [resources.csv](https://www.kaggle.com/c/donorschoose-application-screening/data), we got total of 4 unique features:\n",
    "* id (refer to id in train.csv, can be multiple entries)\n",
    "* description\n",
    "* quantity\n",
    "* price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edb2ea39-c38f-4ebf-aa1f-ed92aee5e8ca",
    "_uuid": "1e0fcaddc831aceaf9af8cf60a18f3e5f15a5f66"
   },
   "source": [
    "## Migrate the features from resource.csv to train.csv\n",
    "We found that the resources.csv is additional features for a project proposal, it includes the resources that request by teachers with details such as desriptions, quantity and price. What we can do is we calculate the total quantity multiply with the price to get the total sum that requested by a project proposal. This amount is then concatenate into the main dataset *df*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6988b2f-f6bc-4e4f-83dd-330a18e6f884",
    "_kg_hide-input": true,
    "_uuid": "5b82e2c225d1747a1c918dd8bbc3ce3c2587277d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resources_df['amount'] = resources_df['quantity']*resources_df['price']\n",
    "amount_df = resources_df.groupby('id')['amount'].agg('sum').sort_values(ascending=False).reset_index()\n",
    "\n",
    "resource_amount_map = {}\n",
    "for i, row in amount_df.iterrows():\n",
    "    resource_amount_map[row['id']] = row['amount']\n",
    "\n",
    "df.insert(4,'total_amount',df['id'].map(resource_amount_map))\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4594449e-07d8-4608-945b-8d09c39cb4c5",
    "_uuid": "315dd53232d056739721e7663d6090a9a7fa11c3"
   },
   "source": [
    "# 2 Analysis\n",
    "This section we will focus on analyzing the data and find out the possible reason behind the rejected proposals. As people have uploaded comprehensive individual feature visualization, we will straight away move into deeper analysis. You can always check other's kernel, e.g [An Educated Guess - DonorsChoose EDA](https://www.kaggle.com/headsortails/an-educated-guess-donorschoose-eda). Also, we will focus on the rate of rejected rather than approved as the ratio of approved is 85:15. The small percentage of rejected proposal is exactly what we going to analyse.\n",
    "\n",
    "# 2.1 Which state has the highest rate of rejected?\n",
    "### Statistics of proposal submission from each state\n",
    "First of all, let's see what is the number of submission from each states, you can always **hover** to see more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf706733-fbae-4055-9103-85f9b7adcfd5",
    "_kg_hide-input": true,
    "_uuid": "0d4f2842cf8598997dfc364ce7fc314525f7e40c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "state_df = df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\n",
    "submit_df = df['school_state'].value_counts().reset_index()\n",
    "submit_df.columns=['school_state','total_submit']\n",
    "submit_df = submit_df.sort_values('school_state').reset_index()\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = state_df['school_state'],\n",
    "        z = submit_df['total_submit'].astype(int),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            )\n",
    "        ),\n",
    "        colorbar = dict(\n",
    "            title = \"Number of Proposal\"\n",
    "        )\n",
    "    ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'US DonorChoose Proposal Submission from States',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "url = py.iplot(fig, filename='donor-state-submit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d8a5982b-6f31-4daa-a668-1cc44093d9a0",
    "_uuid": "550f5c9e8b088da8ca44ace1acbe7c107433b2c5"
   },
   "source": [
    "### Statistics of rate of rejected proposal from each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7e0cdadc-a0c5-4ddd-ad1e-efdc92190c93",
    "_kg_hide-input": true,
    "_uuid": "98c12d009d1603aa8aeaac594398a0f12e11b38e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "state_df = df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\n",
    "submit_df = df['school_state'].value_counts().reset_index()\n",
    "submit_df.columns=['school_state','total_submit']\n",
    "submit_df = submit_df.sort_values('school_state').reset_index()\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = submit_df['school_state'],\n",
    "        z = (submit_df['total_submit'].astype(int)-state_df['project_is_approved'].astype(int))/submit_df['total_submit'].astype(int),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            )\n",
    "        ),\n",
    "        colorbar = dict(\n",
    "            title = \"Rate of Rejected\"\n",
    "        )\n",
    "    ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'US DonorChoose Approval in States',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "url = py.iplot(fig, filename='donor-state-reject')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9aab4a3f-4450-4595-bd62-e65548cc2d4f",
    "_uuid": "0a663d04ac77af0834e6d8b0b3d03e96c1fb3e60"
   },
   "source": [
    "### Observation:\n",
    "* We found that the state that has most submission is **California**(25.7k), follow by **Texas**(12.3k) and **New York**(12.15k).\n",
    "* There is no visible relationship between neighbours of states and number of submissions. \n",
    "* Texas has the highest rate of rejected (18.43%) which above the average (~15%).\n",
    "* All the neighbours of Texas have relatively high rate of rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "866f3ebf-dbe9-4f28-bbdb-beecf5ab6f07",
    "_uuid": "676c8feeff6eb38e397ad1bbc88685391c4f6776"
   },
   "source": [
    "# 2.2 What is the relationship between funding amount and rate of rejected?\n",
    "We usually assume that people will reject proposal that request for excessive amount of money especially from non-profit organization. Will it be true in this case? Let'see.\n",
    "### Average amount of a single funded proposal in states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db59f190-4f04-4fe9-a689-c1fe3727662e",
    "_kg_hide-input": true,
    "_uuid": "be6bca220a1c9754060cd84d53e90d4578db6223",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_df = df[df['project_is_approved']==1]\n",
    "state_df = state_df.groupby('school_state')['project_is_approved'].agg('sum').reset_index().sort_values('school_state').reset_index()\n",
    "a_df = df.groupby('school_state')['total_amount'].agg('sum').reset_index().sort_values('school_state').reset_index()\n",
    "submit_df = df['school_state'].value_counts().reset_index()\n",
    "submit_df.columns=['school_state','total_submit']\n",
    "submit_df = submit_df.sort_values('school_state').reset_index()\n",
    "\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = state_df['school_state'],\n",
    "        z = a_df['total_amount'].astype(int)/state_df['project_is_approved'].astype(int),\n",
    "        #z = (submit_df['total_submit'].astype(int)-state_df['project_is_approved'].astype(int))/submit_df['total_submit'].astype(int),\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            )\n",
    "        ),\n",
    "        colorbar = dict(\n",
    "            title = \"USD\"\n",
    "        )\n",
    "    ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'US DonorChoose Average Funding in States',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "url = py.iplot(fig, filename='donor-state-funding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d850ea03-e047-49aa-ba9a-b8899dc1adb6",
    "_uuid": "de3b143a3dfb60fa41b5f7956bce2d06c93950aa"
   },
   "source": [
    "### Statistic of amount of proposal's request fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0ee226a-d69b-4e96-b944-a8f81c18340a",
    "_kg_hide-input": true,
    "_uuid": "27bb10ebbcc7bdbfbbf9fe6e77280dca74917c51",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['total_amount']=df['total_amount'].apply(lambda x: int(round(x,-2)))\n",
    "fund_total_df = df.groupby('total_amount').count()['project_is_approved'].head(50)\n",
    "fund_total_df = fund_total_df.reset_index()\n",
    "data = [\n",
    "    go.Bar(\n",
    "        x=fund_total_df['total_amount'], # assign x as the dataframe column 'x'\n",
    "        y=fund_total_df['project_is_approved']\n",
    "    )\n",
    "]\n",
    "\n",
    "url = py.iplot(data, filename='fund-total-chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1845b1e-a1d4-4cd2-a658-1645e60a662f",
    "_uuid": "4346307b3b4196eddde3434997b5559dbd14ca73"
   },
   "source": [
    "### Funding Amount v/s Rate of Reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0eef64ef-a3ab-4353-bf9e-695b1a1f0a33",
    "_kg_hide-input": true,
    "_uuid": "d80e8ce89a5038e4da033215faa934884742d921",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fund_df = ((df.groupby('total_amount').count()['project_is_approved']-df.groupby('total_amount')['project_is_approved'].agg('sum'))/df.groupby('total_amount').count()['project_is_approved']).head(50)\n",
    "fund_df = fund_df.reset_index()\n",
    "data = [\n",
    "    go.Bar(\n",
    "        x=fund_df['total_amount'], # assign x as the dataframe column 'x'\n",
    "        y=fund_df['project_is_approved']\n",
    "    )\n",
    "]\n",
    "\n",
    "url = py.iplot(data, filename='fund-reject-chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b48c873-3e2d-4df6-adb6-cb2512c34223",
    "_uuid": "9af6a003928e823e382f31c2ea169e73385d4b48"
   },
   "source": [
    "### Observation:\n",
    "* Most proposal request for fund in between 100USD to 500USD and the number of proposal drop significantly after 2000USD.\n",
    "* Rate of reject increase gradually after 500USD from ~15% to ~20%.\n",
    "* The rate of reject after 2000USD should be consider carefully as the lesser the data, the greater the uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a6af9d2-cb94-407f-9500-24716d64842b",
    "_uuid": "0ae9b433ab954d3382b712f2306938097e45e132"
   },
   "source": [
    "# 2.3 Will grade categories affect the rate of rejected?\n",
    "Since we got 4 categories in grade (3-5, 6-8, 9-12 and PreK-2), let's figure out whether the grades will affect the rate of rejected? \n",
    "### Grades v/s Funding Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af424272-92f0-40cf-87dc-286929d5835d",
    "_kg_hide-input": true,
    "_uuid": "d52f957facf21c35247770e76231b96242cbcf4f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['total_amount'] = df['total_amount'].clip(upper=3000)\n",
    "grade_amount_df = df.groupby(['project_grade_category','total_amount']).count()['id'].unstack().clip(upper=2000)\n",
    "\n",
    "plt.figure(figsize=(20,2))#You can Arrange The Size As Per Requirement\n",
    "ax = sns.heatmap(grade_amount_df, cmap='viridis_r')\n",
    "plt.title(\"Correlation between Grades v/s Funding Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1587e90e-49bd-4797-8cd0-1dbf839d3609",
    "_kg_hide-input": true,
    "_uuid": "f6c41244e09b61ee08457406d0accd275ff7b757",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n",
    "\n",
    "grade_amount_df = df.groupby('project_grade_category').count()['total_amount']\n",
    "grade_amount_df.iplot(kind='bar', yTitle='Number of Proposal', title='Submission from Grade Categories',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ec3c000-4519-43a3-a7d0-4054fb0e8e07",
    "_kg_hide-input": true,
    "_uuid": "272e6932ec116ce60db47154143770be9f61f938",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grade_approval_df = df.groupby('project_grade_category')['project_is_approved'].agg('sum')\n",
    "grade_total_df = df.groupby('project_grade_category').count()['id']\n",
    "((grade_total_df-grade_approval_df)/grade_total_df).iplot(kind='bar', yTitle='Rate of Rejected', title='Rejected Rate of Grade Categories',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe79b52e-9b4f-4aa0-98b4-ccee94c88e77",
    "_uuid": "ee458ed2d7d71d906239754d112289eeaa875554"
   },
   "source": [
    "### Observation:\n",
    "* Grade 3-5 and PreK-2 have the considerably greater amount in proposal submission compare to grade 6-8 and grade 9-12.\n",
    "* However, they have very close rate of rejected which lies in between 14.6% to 16.7%.\n",
    "* This shows that the grade categories is a poor features to training which can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8b3d233-d702-4665-ad6e-7952bdcabd5f",
    "_uuid": "6fe92b87f9de0f06f6518abdf9a0beaa0692175e"
   },
   "source": [
    "# 2.4 Which combination of category and sub-category has highest rate of rejected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bcadeeff-14ca-41ef-aa2d-556342c35f60",
    "_kg_hide-input": true,
    "_uuid": "b8555a05145cbac6772eb66afb7e1882df7f847a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_df = df[['project_subject_categories','project_subject_subcategories','project_is_approved']]\n",
    "new_cat,new_sub,new_approve = [],[],[]\n",
    "\n",
    "for i, row in cat_df.iterrows():\n",
    "    cats = row['project_subject_categories'].split(', ')\n",
    "    subs = row['project_subject_subcategories'].split(', ')\n",
    "    for j in range(len(cats)):\n",
    "        for k in range(len(subs)):\n",
    "            new_cat.append(cats[j])\n",
    "            new_sub.append(subs[k])\n",
    "            new_approve.append(row['project_is_approved'])\n",
    "            \n",
    "new_cat = pd.DataFrame({'project_subject_categories':new_cat})\n",
    "new_sub = pd.DataFrame({'project_subject_subcategories':new_sub})\n",
    "new_approve = pd.DataFrame({'project_is_approved':new_approve})\n",
    "\n",
    "cat_total_df = pd.concat([new_cat,new_sub,new_approve],axis=1).reset_index()\n",
    "cat_approval_df = cat_total_df.groupby(['project_subject_categories','project_subject_subcategories'])['project_is_approved'].agg('sum')\n",
    "cat_all_df = cat_total_df.groupby(['project_subject_categories','project_subject_subcategories']).count()['project_is_approved']\n",
    "cat_heat = (cat_approval_df/cat_all_df).unstack()\n",
    "\n",
    "plt.figure(figsize=(18,5))#You can Arrange The Size As Per Requirement\n",
    "ax = sns.heatmap(cat_heat, cmap='viridis_r')\n",
    "plt.title(\"Aprroved rate for categories and sub-categories combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5df18fa-73c5-4e2c-8fd4-2ad50fdb8470",
    "_uuid": "04c73ad341817f23e5cdf1afc076a4cc32467a7c",
    "collapsed": true
   },
   "source": [
    "### Observation:\n",
    "* There are total 9x30 combinations of categories and sub-categories.\n",
    "* Top 3 categories that consistent the most in the rate of approved is Applied Learning, Literacy & Language and Math & Science.\n",
    "* Both Warmth and Care & Hunger have very extreme rate of approved and rejected on certain sub-categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ef8a773-c6e9-4216-bf41-ea8ad14d0ba8",
    "_uuid": "d22ff9ac4a35fc6148712457a989bd74458c8d83"
   },
   "source": [
    "# 2.5 What is the relationship between essay sentiment and rate of rejected?\n",
    "In this section, we going to analyze:\n",
    "* The relationship between length of sentences in essays and rate of rejected\n",
    "* The relationship between essay sentiment (Polarity and Subjectivity) and rate of rejected\n",
    "\n",
    "### Length of Sentences in Essays v/s Rate of Rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e054ad6a-13d4-476e-9ee9-a90ad60a6a88",
    "_kg_hide-input": true,
    "_uuid": "bc89c7a91b6237e66d7cb619705670e598220705",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def count_sent(text):\n",
    "    if text == \"nan\" or not text:\n",
    "        return 0\n",
    "    sents = sent_tokenize(text)\n",
    "    return len(sents)\n",
    "\n",
    "sent_df = df[['project_essay_1','project_is_approved']]\n",
    "sent_df['sent_length'] = sent_df['project_essay_1'].apply(count_sent)\n",
    "\n",
    "apr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\n",
    "tot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\n",
    "rat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\n",
    "rat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_1 v/s Rate of Rejected',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e7ffe70-707f-49ad-aaec-dfdd4a38b3f5",
    "_kg_hide-input": true,
    "_uuid": "ee44780f74eff39fccd38551a1969121680ade57",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_df = df[['project_essay_2','project_is_approved']]\n",
    "sent_df['project_essay_2'] = sent_df['project_essay_2'].astype(str)\n",
    "sent_df['sent_length'] = sent_df['project_essay_2'].apply(count_sent)\n",
    "\n",
    "apr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\n",
    "tot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\n",
    "rat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\n",
    "rat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_2 v/s Rate of Rejected',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09bc2f02-8e84-4a40-89ac-771a2f842d31",
    "_kg_hide-input": true,
    "_uuid": "32ddc193efe823051ef5b892326fdd1ffcefb177",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_df = df[['project_essay_3','project_is_approved']]\n",
    "sent_df['project_essay_3'] = sent_df['project_essay_3'].astype(str)\n",
    "sent_df['sent_length'] = sent_df['project_essay_3'].apply(count_sent)\n",
    "\n",
    "apr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\n",
    "tot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\n",
    "rat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\n",
    "rat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_3 v/s Rate of Rejected',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "81c43e29-c9d1-4819-905d-30ea6095ac11",
    "_kg_hide-input": true,
    "_uuid": "70f5a26316b6d69892a4063e1c7314102d91bf0b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_df = df[['project_essay_4','project_is_approved']]\n",
    "sent_df['project_essay_4'] = sent_df['project_essay_4'].astype(str)\n",
    "sent_df['sent_length'] = sent_df['project_essay_4'].apply(count_sent)\n",
    "\n",
    "apr = sent_df.groupby('sent_length')['project_is_approved'].agg('sum').reset_index().sort_values('sent_length')\n",
    "tot = sent_df.groupby('sent_length').count()['project_is_approved'].reset_index().sort_values('sent_length')\n",
    "rat = (tot['project_is_approved']-apr['project_is_approved'])/tot['project_is_approved']\n",
    "rat.iplot(kind='bar', yTitle='Number of Sentences', title='Length of Sentences in Essay_4 v/s Rate of Rejected',\n",
    "             filename='Grade categorical-bar-chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cb18c9a-dddf-4b21-817b-ecf51bd3a3e6",
    "_uuid": "4f01821b0a5ac819e07b43d916b8f997bcc05247"
   },
   "source": [
    "### Observation :\n",
    "* There is no significant correlation in between length of sentences and rate of rejected.\n",
    "* Both essay_3 and essay_4 leave unfill by most of the project proposal.\n",
    "* High rate of rejected is observed in proposal with 0 sentences in essay_2 (~32%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa97791f-44e3-47cd-a8b6-95c55ee070a9",
    "_uuid": "27665b3569fc90b87cddfa7070a3be6b0b4eb771"
   },
   "source": [
    "# 2.5 What is the relationship between essay sentiment and rate of rejected?\n",
    "In this section we going to analyze how essay's sentiment (polarity and subjectivity) will affect rate of rejected. First of all, we concatenate all the essay_1 to essay_4. Then, textblob sentiment is used to get the level of polarity and subjectivity for each project proposal. Below is the plotted heatmap that shows what essay sentiment have highest chance to be rejected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "017928ac-ccb6-4235-83c5-51e8acde9829",
    "_uuid": "43365f154a3b596892e4c1fece26bdf9fa2335da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "df[\"essay\"] = df[\"project_essay_1\"].map(str) + df[\"project_essay_2\"].map(str) + df[\"project_essay_3\"].map(str) + df[\"project_essay_4\"].map(str)\n",
    "def get_polarity(text):\n",
    "    textblob = TextBlob(text)\n",
    "    pol = textblob.sentiment.polarity\n",
    "    return round(pol,2)\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    textblob = TextBlob(text)\n",
    "    subj = textblob.sentiment.subjectivity\n",
    "    return round(subj,2)\n",
    "\n",
    "pol_df = df[['essay','project_is_approved']]\n",
    "pol_df['sent_polarity'] = pol_df['essay'].apply(get_polarity)\n",
    "pol_df['sent_subjectivity'] = pol_df['essay'].apply(get_subjectivity)\n",
    "\n",
    "pol_rj_df = pol_df[pol_df['project_is_approved']==0]\n",
    "#sub_df = df[['project_essay_1','project_is_approved']]\n",
    "#sub_df['project_essay_1'] = sub_df['project_essay_1'].astype(str)\n",
    "#sub_df['sent_subjectivity'] = sub_df['project_essay_1'].apply(subjectivity)\n",
    "\n",
    "pol_total_df = pol_df.groupby(['sent_polarity','sent_subjectivity']).count()['project_is_approved'].unstack().clip(upper=5000)\n",
    "pol_rj_df = pol_rj_df.groupby(['sent_polarity','sent_subjectivity']).count()['project_is_approved'].unstack().clip(upper=5000)\n",
    "pol_rat_df = pol_rj_df/pol_total_df\n",
    "#pol_heat_df\n",
    "plt.figure(figsize=(20,20))#You can Arrange The Size As Per Requirement\n",
    "ax = sns.heatmap(pol_rat_df, cmap='viridis_r')\n",
    "plt.title(\"Correlation between Polarity and Subjectivity for Rate of\")\n",
    "#sent_df\n",
    "#apr = sent_df.groupby('sent_polarity')['project_is_approved'].agg('sum').reset_index()\n",
    "#tot = sent_df.groupby('sent_polarity').count()['project_is_approved'].reset_index()\n",
    "#rat = apr['project_is_approved']/tot['project_is_approved']\n",
    "#rat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64d740c5-c43a-4aea-8a58-2f743688983a",
    "_uuid": "544749db1f3eb7723372dfde228861cc9c9c2a0a"
   },
   "source": [
    "### Observation :\n",
    "* The essay in centre of sentiment heatmap has the lowest rate of rejected\n",
    "* They rejected whatever proposal that subjectivity(greater than 0.78 , less than 0.18) and polarity(less than0.08 , greater than 0.58)\n",
    "* The rate of rejected increase gradually from centre of heatmap to periphery of heatmap.\n",
    "\n",
    "# 3 Suggestion\n",
    "* From this kernel, we shows that not all the features will fit your training model, some are considerably flat and might be redundant to your model.\n",
    "* Features that suggested is **Categories & Sub-categories** and ** Sentiment of Essay**.\n",
    "* DonorChoose.org can use these features as pre-screen to allocate volunteer effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e626000-6a3a-4a3d-af07-1c8866d327b5",
    "_uuid": "288d6d8064a993d997e65f47f2cfd21be8159fb6"
   },
   "source": [
    "# 4 LightGBM + TFIDF\n",
    "First of all, we need to thank **Oleg Panichev ** for his [LightGBM and Tf-idf Starter](https://www.kaggle.com/opanichev/lightgbm-and-tf-idf-starter/code). We implement his model but we have modified it by adding and removing some features as mentioned in EDA above.\n",
    "1. We removed the teacher_prefix and grade_categories\n",
    "2. We included the sentiment analysis (Polarity and Subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf5e0de8-b455-4d9b-9833-91279499cbe6",
    "_kg_hide-output": true,
    "_uuid": "b601f493ead568ccb1ab79b5d66a843d4872723f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Load Data\n",
    "dtype = {\n",
    "    'id': str,\n",
    "    'teacher_id': str,\n",
    "    'teacher_prefix': str,\n",
    "    'school_state': str,\n",
    "    'project_submitted_datetime': str,\n",
    "    'project_grade_category': str,\n",
    "    'project_subject_categories': str,\n",
    "    'project_subject_subcategories': str,\n",
    "    'project_title': str,\n",
    "    'project_essay_1': str,\n",
    "    'project_essay_2': str,\n",
    "    'project_essay_3': str,\n",
    "    'project_essay_4': str,\n",
    "    'project_resource_summary': str,\n",
    "    'teacher_number_of_previously_posted_projects': int,\n",
    "    'project_is_approved': np.uint8,\n",
    "}\n",
    "data_path = os.path.join('..', 'input')\n",
    "train = pd.read_csv(os.path.join(data_path, 'train.csv'), dtype=dtype, low_memory=True)\n",
    "test = pd.read_csv(os.path.join(data_path, 'test.csv'), dtype=dtype, low_memory=True)\n",
    "res = pd.read_csv(os.path.join(data_path, 'resources.csv'))\n",
    "\n",
    "print(train.head())\n",
    "# print(test.head())\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "train['project_essay'] = train.apply(lambda row: ' '.join([\n",
    "    str(row['teacher_prefix']), \n",
    "    str(row['school_state']), \n",
    "    str(row['project_grade_category']), \n",
    "    str(row['project_subject_categories']), \n",
    "    str(row['project_subject_subcategories']), \n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4']),\n",
    "    ]), axis=1)\n",
    "test['project_essay'] = test.apply(lambda row: ' '.join([\n",
    "    str(row['teacher_prefix']), \n",
    "    str(row['school_state']), \n",
    "    str(row['project_grade_category']), \n",
    "    str(row['project_subject_categories']), \n",
    "    str(row['project_subject_subcategories']), \n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4']),\n",
    "    ]), axis=1)\n",
    "\n",
    "def extract_features(df):\n",
    "    df['project_title_len'] = df['project_title'].apply(lambda x: len(str(x)))\n",
    "    df['project_essay_1_len'] = df['project_essay_1'].apply(lambda x: len(str(x)))\n",
    "    df['project_essay_2_len'] = df['project_essay_2'].apply(lambda x: len(str(x)))\n",
    "    df['project_essay_3_len'] = df['project_essay_3'].apply(lambda x: len(str(x)))\n",
    "    df['project_essay_4_len'] = df['project_essay_4'].apply(lambda x: len(str(x)))\n",
    "    df['project_resource_summary_len'] = df['project_resource_summary'].apply(lambda x: len(str(x)))\n",
    "  \n",
    "extract_features(train)\n",
    "extract_features(test)\n",
    "\n",
    "from textblob import TextBlob\n",
    "def get_polarity(text):\n",
    "    textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "    pol = textblob.sentiment.polarity\n",
    "    return round(pol,3)\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "    subj = textblob.sentiment.subjectivity\n",
    "    return round(subj,3)\n",
    "\n",
    "train['polarity'] = train['project_essay'].apply(get_polarity)\n",
    "train['subjectivity'] = train['project_essay'].apply(get_subjectivity)\n",
    "test['polarity'] = test['project_essay'].apply(get_polarity)\n",
    "test['subjectivity'] = test['project_essay'].apply(get_subjectivity)\n",
    "\n",
    "train = train.drop([\n",
    "    'project_essay_1', \n",
    "    'project_essay_2', \n",
    "    'project_essay_3', \n",
    "    'project_essay_4'], axis=1)\n",
    "test = test.drop([\n",
    "    'project_essay_1', \n",
    "    'project_essay_2', \n",
    "    'project_essay_3', \n",
    "    'project_essay_4'], axis=1)\n",
    "\n",
    "df_all = pd.concat([train, test], axis=0)\n",
    "gc.collect()\n",
    "\n",
    "# Merge with resources\n",
    "res = pd.DataFrame(res[['id', 'price']].groupby('id').price.agg(\\\n",
    "    [\n",
    "        'count', \n",
    "        'sum', \n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std', \n",
    "        # 'median',\n",
    "        lambda x: len(np.unique(x)),\n",
    "    ])).reset_index()\n",
    "print(res.head())\n",
    "train = train.merge(res, on='id', how='left')\n",
    "test = test.merge(res, on='id', how='left')\n",
    "del res\n",
    "gc.collect()\n",
    "\n",
    "# Preprocess columns with label encoder\n",
    "print('Label Encoder...')\n",
    "cols = [\n",
    "    'teacher_id', \n",
    "    'school_state', \n",
    "    'project_subject_categories', \n",
    "    'project_subject_subcategories'\n",
    "]\n",
    "\n",
    "for c in tqdm(cols):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_all[c].astype(str))\n",
    "    train[c] = le.transform(train[c].astype(str))\n",
    "    test[c] = le.transform(test[c].astype(str))\n",
    "del le\n",
    "gc.collect()\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Preprocess timestamp\n",
    "print('Preprocessing timestamp...')\n",
    "\n",
    "train['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime']).values.astype(np.int64)\n",
    "test['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime']).values.astype(np.int64)\n",
    "print('Done.')\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "print('Preprocessing text...')\n",
    "cols = [\n",
    "    'project_title', \n",
    "    'project_essay', \n",
    "    'project_resource_summary'\n",
    "]\n",
    "n_features = [\n",
    "    400, \n",
    "    5000, \n",
    "    400\n",
    "]\n",
    "\n",
    "for c_i, c in tqdm(enumerate(cols)):\n",
    "    tfidf = TfidfVectorizer(max_features=n_features[c_i], min_df=3)\n",
    "    tfidf.fit(df_all[c])\n",
    "    tfidf_train = np.array(tfidf.transform(train[c]).todense(), dtype=np.float16)\n",
    "    tfidf_test = np.array(tfidf.transform(test[c]).todense(), dtype=np.float16)\n",
    "\n",
    "    for i in range(n_features[c_i]):\n",
    "        train[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "        test[c + '_tfidf_' + str(i)] = tfidf_test[:, i]\n",
    "        \n",
    "    del tfidf, tfidf_train, tfidf_test\n",
    "    gc.collect()\n",
    "    \n",
    "print('Done.')\n",
    "del df_all\n",
    "gc.collect()\n",
    "\n",
    "# Prepare data\n",
    "cols_to_drop = [\n",
    "    'id',\n",
    "    'project_title', \n",
    "    'project_essay', \n",
    "    'project_resource_summary',\n",
    "    'project_is_approved',\n",
    "]\n",
    "X = train.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "y = train['project_is_approved']\n",
    "X_test = test.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "id_test = test['id'].values\n",
    "feature_names = list(X.columns)\n",
    "print(X.shape, X_test.shape)\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "# Build the model\n",
    "cnt = 0\n",
    "p_buf = []\n",
    "n_splits = 5\n",
    "n_repeats = 1\n",
    "kf = RepeatedKFold(\n",
    "    n_splits=n_splits, \n",
    "    n_repeats=n_repeats, \n",
    "    random_state=0)\n",
    "auc_buf = []   \n",
    "\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    print('Fold {}/{}'.format(cnt + 1, n_splits))\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 16,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.025,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'num_threads': 1,\n",
    "        'lambda_l2': 1,\n",
    "        'min_gain_to_split': 0,\n",
    "    }  \n",
    "\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=feature_names),\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    if cnt == 0:\n",
    "        importance = model.feature_importance()\n",
    "        model_fnames = model.feature_name()\n",
    "        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n",
    "        tuples = [x for x in tuples if x[1] > 0]\n",
    "        print('Important features:')\n",
    "        print(tuples[:50])\n",
    "\n",
    "    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y.loc[valid_index], p)\n",
    "\n",
    "    print('{} AUC: {}'.format(cnt, auc))\n",
    "\n",
    "    p = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p)\n",
    "    else:\n",
    "        p_buf += np.array(p)\n",
    "    auc_buf.append(auc)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt > 0: # Comment this to run several folds\n",
    "        break\n",
    "    \n",
    "    del model\n",
    "    gc.collect\n",
    "\n",
    "auc_mean = np.mean(auc_buf)\n",
    "auc_std = np.std(auc_buf)\n",
    "print('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n",
    "\n",
    "preds = p_buf/cnt\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = id_test\n",
    "subm['project_is_approved'] = preds\n",
    "subm.to_csv('submission.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dbe0596e-531a-4129-bb0a-a4af0a4220ac",
    "_uuid": "94940981025663f515ce86f7818877c043df0ac1"
   },
   "source": [
    "# 5 GRU-ATT\n",
    "Then, we introduce another model called GRU-ATT network. It's come from our machine learning research and will get publish soon. This model is serve for text classification and it got state-of-art performance in some dataset. We comment all the code as it will exceed the running time allow by Kaggle notebook. Let's see what's the output if we apply this model to DonorChoose data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a5e9b86-f489-410a-ac50-2d20ba288291",
    "_kg_hide-output": true,
    "_uuid": "b23f106c42b7ea14dcecabef7bd4331fc6400e2a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Author-Poon\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 50\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)   \n",
    "    return string.strip().lower()\n",
    "\n",
    "dtype = {\n",
    "    'id': str,\n",
    "    'teacher_id': str,\n",
    "    'teacher_prefix': str,\n",
    "    'school_state': str,\n",
    "    'project_submitted_datetime': str,\n",
    "    'project_grade_category': str,\n",
    "    'project_subject_categories': str,\n",
    "    'project_subject_subcategories': str,\n",
    "    'project_title': str,\n",
    "    'project_essay_1': str,\n",
    "    'project_essay_2': str,\n",
    "    'project_essay_3': str,\n",
    "    'project_essay_4': str,\n",
    "    'project_resource_summary': str,\n",
    "    'teacher_number_of_previously_posted_projects': int,\n",
    "    'project_is_approved': np.uint8,\n",
    "}\n",
    "#data_path = os.path.join('..', 'input')\n",
    "train = pd.read_csv('train.csv', dtype=dtype, low_memory=True)\n",
    "test = pd.read_csv( 'test.csv', dtype=dtype, low_memory=True)\n",
    "\n",
    "test['project_is_approved'] = 1\n",
    "\n",
    "train['text'] = train.apply(lambda row: ' '.join([\n",
    "    str(row['project_title']), \n",
    "    str(row['project_resource_summary']), \n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4'])]), axis=1)\n",
    "test['text'] = test.apply(lambda row: ' '.join([\n",
    "    str(row['project_title']), \n",
    "    str(row['project_resource_summary']), \n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "train = train.drop([\n",
    "    'teacher_id',\n",
    "    'teacher_prefix',\n",
    "    'school_state',\n",
    "    'project_submitted_datetime',\n",
    "    'project_grade_category',\n",
    "    'project_subject_categories',\n",
    "    'project_subject_subcategories',\n",
    "    'project_title',\n",
    "    'project_essay_1',\n",
    "    'project_essay_2',\n",
    "    'project_essay_3',\n",
    "    'project_essay_4',\n",
    "    'project_resource_summary',\n",
    "    'teacher_number_of_previously_posted_projects'], axis=1)\n",
    "test = test.drop([\n",
    "    'teacher_id',\n",
    "    'teacher_prefix',\n",
    "    'school_state',\n",
    "    'project_submitted_datetime',\n",
    "    'project_grade_category',\n",
    "    'project_subject_categories',\n",
    "    'project_subject_subcategories',\n",
    "    'project_title',\n",
    "    'project_essay_1',\n",
    "    'project_essay_2',\n",
    "    'project_essay_3',\n",
    "    'project_essay_4',\n",
    "    'project_resource_summary',\n",
    "    'teacher_number_of_previously_posted_projects'], axis=1)\n",
    "\n",
    "data_train = pd.concat([train,test],axis = 0).reset_index()\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "instance_inputs = []\n",
    "comment_id = []\n",
    "\n",
    "#Return dimension of data_train.review([0]=row)\n",
    "for idx in range(data_train.text.shape[0]):\n",
    "    sys.stdout.write(\"\\rProcessing ---- %d\"%idx)\n",
    "    sys.stdout.flush()\n",
    "    comment_id.append(data_train.id[idx])\n",
    "    text = ''.join(data_train.text[idx])\n",
    "    #parse the sentences into beautifulsoup object\n",
    "    #print text\n",
    "    text = BeautifulSoup(text)\n",
    "    text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "    #insert clear text into texts array\n",
    "    texts.append(text)\n",
    "    #Return a sentence-tokenized copy of text( divide string into substring by punkt)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "    labels.append(data_train.project_is_approved[idx])\n",
    "\n",
    "#Class for vectorizing texts (Tokenizer)\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "#list of texts to train on\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "#New 3D array filled with zero with (length,15,100) length= num of char\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "word_len = np.zeros(10000)\n",
    "#enumerate produce a tuple(index)\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "\tword_len[len(sentences)] +=1\n",
    "        if j< MAX_SENTS:\n",
    "\t    #Split sentence into a list of words\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "\t\t    #dictionary mapping word to their rank/index (int)\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1                    \n",
    "                    \n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "#Converts a class vector (integers) to binary class matrix\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "nb_validation_samples = 78035\n",
    "#split training and validation set\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "comment_id = comment_id[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print y_train.sum(axis=0)\n",
    "print y_val.sum(axis=0)\n",
    "\n",
    "GLOVE_DIR = \"../../Glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    #split the vector of 100d\n",
    "    values = line.split()\n",
    "    #word at values[0]\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                  shape=(input_shape[-1],),\n",
    "                                  initializer='normal',\n",
    "                                  trainable=True)\n",
    "        super(AttLayer, self).build(input_shape) \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))     \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='main_input')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "print model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=3, batch_size=100, verbose=2)\n",
    "\n",
    "score = model.evaluate(data, labels, batch_size = 100, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n",
    "yFit = model.predict(data, batch_size = 100, verbose=2)\n",
    "yFit = yFit[-nb_validation_samples:,1]\n",
    "val_pre = pd.DataFrame({'project_is_approved':yFit})\n",
    "val_id = pd.DataFrame({'id':comment_id})\n",
    "data_test_merged = pd.concat([val_id,val_pre], axis=1)\n",
    "data_test_merged.to_csv('GRU.csv', encoding='utf-8', index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e0d8fba-7c27-459a-9b4b-7c3e6d59635a",
    "_uuid": "2d542da8eec403bb35bb4761886e9c7c185d5971"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc1c2722-3109-4574-9810-f56131a12ea2",
    "_uuid": "0105f4e41db10bd58d459299d695376ce698e83d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LGBM = pd.read_csv('../input/submit/LGBMTFIDF.csv')\n",
    "GRU = pd.read_csv('../input/submit/GRU.csv')\n",
    "combine = pd.read_csv('../input/submit/submission_combine7.csv')\n",
    "\n",
    "res_df = pd.concat([LGBM['project_is_approved'],GRU['project_is_approved'],combine['project_is_approved']],axis=1)\n",
    "res_df.columns=['LGBMTDIDF','GRU','Combined']\n",
    "\n",
    "data = []\n",
    "for col in res_df.columns:\n",
    "    data.append(  go.Box( y=res_df[col], name=col, showlegend=False ) )\n",
    "    \n",
    "data.append( go.Scatter( x = res_df.columns, y = res_df.mean(), mode='lines', name='mean' ) )\n",
    "\n",
    "# IPython notebook\n",
    "# py.iplot(data, filename='pandas-box-plot')\n",
    "\n",
    "url = py.iplot(data, filename='pandas-box-plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2dc2eced-4c9b-47d5-bb3c-434e9f4886d1",
    "_uuid": "285fd5165a6117b3e03fb333bea656642c41ea1f"
   },
   "source": [
    "From the box plot we can see the result from LGBM-TFIDF has the higher mean compare to result from GRU which shows that GRU is more distributed in comparison. By combining result from both model with weights, we can have advantages from both model and a more balanced result.\n",
    "## Result submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f16d24b5-8b52-45a9-bf11-3dca5c93633a",
    "_uuid": "94ad46d9120b80d8af3c068a85a7812452ad7faa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv('../input/submit/submission_combine7.csv')\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59758b3c-7965-4c55-899c-4e6e8e8d9d52",
    "_uuid": "51bbc440e76528c2518ccbbdd9ce152516046057"
   },
   "source": [
    "----\n",
    "# Thanks for watching. We are new to Kaggle, if you find this kernel is helpful, please **VOTE** for our kernel. Your support is Greatly appreciate!!! Also, please feel free to drop us any question or comment.\n",
    "\n",
    "# To be continued.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
